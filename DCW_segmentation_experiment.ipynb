{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from readImages import *\n",
    "from imageModifiers import *\n",
    "from readDataFiles import *\n",
    "from projEdgeBreaks import *\n",
    "from gaussBreaks import *\n",
    "from saveImages import *\n",
    "from plottingFuncs import *\n",
    "\n",
    "# change pandas parameter\n",
    "pd.options.mode.chained_assignment = None\n",
    "# change plot size\n",
    "mpl.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify file and folder locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjFile = \"data/decoding-the-civil-war-subjects-9-29-17.csv\"\n",
    "clFile = 'data/letter-slope-classifications_10nov2017.csv'\n",
    "savefile = \"accuracy.pkl\"\n",
    "\n",
    "consensusFolder = \"C:/Users/danny/Repos/text_segmentation/consensus/consensus/\"\n",
    "# consensusFile = consensusFolder + \"decoding-the-civil-war-consensus-linewise_{mss_label}.csv\"\n",
    "consensusCsvFiles = glob.glob('{}/*.csv'.format(consensusFolder))\n",
    "consensusCsvFiles = [i for i in consensusCsvFiles if \"linewise\" in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run segmentation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matchLines(lb, actuallb, yoffset, matchlim = 30):\n",
    "    matches = []\n",
    "    lb2 = np.add(lb, yoffset)\n",
    "    for i in range(len(lb)):\n",
    "        closest = np.argmin(np.abs(np.subtract(lb2[i], actuallb)))\n",
    "        if np.abs(lb2[i] - actuallb[closest]) < matchlim:\n",
    "            matches.append(closest)\n",
    "        else:\n",
    "            matches.append(-1)\n",
    "    return matches\n",
    "\n",
    "\n",
    "nuOpt = np.arange(0.5, 6.5, 0.5) # 0.5:6.5\n",
    "biOpt = list(range(1, 10)) # 1:10\n",
    "\n",
    "def wordBreaks(grey, lb, matches, linesForTele, nuOpt=nuOpt, biOpt=biOpt):\n",
    "    accurL = []\n",
    "\n",
    "    for i in range(1, len(lb)):\n",
    "        print(i, end=\" \")\n",
    "        if matches[i] == -1:\n",
    "            continue\n",
    "        chunk = grey[lb[i-1]:lb[i],]\n",
    "        rw = linesForTele.loc[matches[i],\"consensus_text\"]\n",
    "        # remove leading and trailing \"\n",
    "        if rw[0] == '\"':\n",
    "            rw = rw[1:]\n",
    "        if rw[-1] == '\"':\n",
    "            rw = rw[:-1]\n",
    "        nr = len(rw.split(\" \"))\n",
    "        for nu in nuOpt:\n",
    "            for bi in biOpt:\n",
    "                br = gaussBreaks(chunk, nu=nu, biThresh=bi, shear=0, fix=0)\n",
    "                accurL.append({\"lb_index\":i, \"nu\":nu, \"biThresh\":bi, \"n_words_cons\":nr,\n",
    "                               \"breaks\":br, \"n_words_gauss\": len(br)})\n",
    "    return pd.DataFrame(accurL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this to only do one file\n",
    "# consensusCsvFiles = [consensusCsvFiles[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mssEC_02_007\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 "
     ]
    }
   ],
   "source": [
    "# subject file\n",
    "subj = readSubjFile(subjFile)\n",
    "# classification export\n",
    "clExp = readSlopeClassification(clFile)\n",
    "\n",
    "\n",
    "data = {}\n",
    "for consensusFile in consensusCsvFiles:\n",
    "    # consensusFile = consensusCsvFiles[0]\n",
    "    #################################################################################\n",
    "    # Read data files\n",
    "    # consensus file (by line)\n",
    "    cons = readConsensusFile(consensusFile)\n",
    "    # combine data files\n",
    "    idAndUrl, telegrams = mergeSubjAndCons(subj, cons, clExp)\n",
    "\n",
    "    ##############################################################################################\n",
    "    # collect the data for the wordbreaks\n",
    "\n",
    "    for im in list(idAndUrl.index):\n",
    "    #     im = list(idAndUrl.index)[0]\n",
    "        hdl_id = idAndUrl.loc[im, \"hdl_id\"]\n",
    "        data[hdl_id] = {}\n",
    "        print(hdl_id)\n",
    "\n",
    "        data[hdl_id][\"url\"] = idAndUrl.loc[im, \"url_cons\"]\n",
    "        linesForTele = telegrams.loc[telegrams[\"hdl_id\"] == hdl_id]\n",
    "        linesForTele.loc[:,\"y1\"] = [np.mean(eval(l))\n",
    "                                    for l in linesForTele.loc[:,\"y_loc\"]]\n",
    "\n",
    "        # read in and do all pre-processing #################################\n",
    "        let_orig, grey_orig = readImg(idAndUrl.loc[im, \"url_cons\"])\n",
    "        grey, let, offset = removeEdges(grey_orig, let_orig, 35.0)\n",
    "        greySm = smoothImg(grey, 7.0)\n",
    "\n",
    "        # get linebreaks ####################################################\n",
    "        matchlim = 30\n",
    "        lb = projBreaks(greySm, \"y\")\n",
    "        lb = filterBreaks(lb, matchlim)\n",
    "        data[hdl_id][\"lb\"] = lb\n",
    "\n",
    "        # get matching lines with actual lines ##############################\n",
    "        actuallb = linesForTele.loc[:, 'y1']\n",
    "        data[hdl_id]['trim_offset'] = offset\n",
    "        matches = matchLines(lb, actuallb, offset[0])\n",
    "        data[hdl_id][\"matches\"] = matches\n",
    "\n",
    "        # get wordbreaks ####################################################\n",
    "        accur = wordBreaks(grey, lb, matches, linesForTele)\n",
    "        data[hdl_id][\"segment_results\"] = accur\n",
    "        print()\n",
    "\n",
    "#         save object\n",
    "        with open(savefile, \"wb\") as f:\n",
    "            pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordBreaks(grey, lb, matches, linesForTele, nuOpt=nuOpt, biOpt=biOpt):\n",
    "    accurL = []\n",
    "\n",
    "    for i in range(1, len(lb)):\n",
    "        print(i, end=\" \")\n",
    "        if matches[i] == -1:\n",
    "            continue\n",
    "        chunk = grey[lb[i-1]:lb[i],]\n",
    "        rw = linesForTele.loc[matches[i],\"consensus_text\"]\n",
    "        # remove leading and trailing \"\n",
    "        if rw[0] == '\"':\n",
    "            rw = rw[1:]\n",
    "        if rw[-1] == '\"':\n",
    "            rw = rw[:-1]\n",
    "        nr = len(rw.split(\" \"))\n",
    "        nu = 3.5\n",
    "        bi = 2\n",
    "        br = gaussBreaks(chunk, nu=nu, biThresh=bi, shear=0, fix=0)\n",
    "        accurL.append({\"lb_index\":i, \"nu\":nu, \"biThresh\":bi, \"n_words_cons\":nr,\n",
    "                       \"breaks\":br, \"n_words_gauss\": len(br)})\n",
    "    return pd.DataFrame(accurL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
