{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from scipy import stats\n",
    "from scipy.ndimage.filters import gaussian_filter as gf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Rectangle as Rec\n",
    "\n",
    "from skimage import filters\n",
    "from skimage import transform as tf\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import urllib\n",
    "from PIL import Image\n",
    "\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "from projEdgeBreaks import *\n",
    "from imageModifiers import *\n",
    "from saveImages import *\n",
    "from plottingFuncs import *\n",
    "\n",
    "# change pandas parameter\n",
    "pd.options.mode.chained_assignment = None\n",
    "# change plot size\n",
    "mpl.rcParams[\"figure.figsize\"] = (15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussBreaks(chunk, nu=3.5, biThresh=2, shear=0.6, fix=15, plotIt=False):\n",
    "    # get smoothing factors\n",
    "    sigYs = np.arange(1, 8, 0.3)\n",
    "    sigXs = sigYs * nu\n",
    "    # shear the image\n",
    "    oldrg = (np.max(chunk) - np.min(chunk))\n",
    "    newrg = 2\n",
    "    ch2 = (((chunk - np.min(chunk)) * newrg) / oldrg) - 1\n",
    "    mytf = tf.AffineTransform(shear=shear)\n",
    "    chunk = tf.warp(ch2, inverse_map=mytf)\n",
    "    \n",
    "    # choose which smoothing factor to use based on minimizing\n",
    "    # the white space\n",
    "    extents = []\n",
    "    count = 0\n",
    "    for j in range(len(sigYs)):\n",
    "        filt = gf(input=chunk, sigma=(sigYs[j],sigXs[j]), order=0)\n",
    "        if count < biThresh:\n",
    "            th = filters.threshold_otsu(filt)\n",
    "            count += 1\n",
    "        binfilt = binarizeImg(filt, th)\n",
    "        extents.append(np.sum(binfilt))\n",
    "    j = np.argmin(extents)\n",
    "    \n",
    "    filt = ndimage.filters.gaussian_filter(input=chunk, sigma=(sigYs[j],sigXs[j]))\n",
    "    binfilt = 1 - binarizeImg(filt, th)\n",
    "\n",
    "    # find connect components\n",
    "    labels, nrObj = ndimage.label(binfilt)\n",
    "    osli = ndimage.find_objects(labels)\n",
    "    \n",
    "    # find the word boxes\n",
    "    rec = []\n",
    "    bounds = []\n",
    "    sh = np.max(labels.shape)\n",
    "    for sl in osli:\n",
    "        sl0 = sl[0].indices(sh)\n",
    "        sl1 = sl[1].indices(sh)\n",
    "        \n",
    "        xLeng = sl1[1]-sl1[0]\n",
    "        yLeng = sl0[1]-sl0[0]\n",
    "        if xLeng*yLeng > 100:\n",
    "            bounds.append([sl1[0], sl1[1]])\n",
    "            rec.append([[sl1[0], sl0[0]], xLeng, yLeng])\n",
    "    # combine those that are surrounded by others\n",
    "    bounds = sorted(bounds)\n",
    "    newbounds = []\n",
    "    skipnext = False\n",
    "    if len(bounds) <= 0:\n",
    "        return [0, chunk.shape[1]]\n",
    "    bPrev = bounds[0]\n",
    "    for i in range(1, len(bounds)):\n",
    "        bCur = bounds[i]\n",
    "        if bPrev[1] > bCur[0]:\n",
    "            bPrev = [bPrev[0], bCur[1]]\n",
    "        else:\n",
    "            newbounds.append(bPrev)\n",
    "            bPrev = [x for x in bCur]\n",
    "    newbounds.append(bPrev)\n",
    "    try:\n",
    "        wbLine = [newbounds[0][0]]\n",
    "    except:\n",
    "        wbLine = [0]\n",
    "    for i in range(1, len(newbounds)):\n",
    "        b1 = newbounds[i-1]\n",
    "        b2 = newbounds[i]\n",
    "        wbLine.append(np.mean([b1[1], b2[0]])-fix)\n",
    "    try:\n",
    "        wbLine.append(b2[1])\n",
    "    except:\n",
    "        wbLine.append(chunk.shape[1])\n",
    "#         pass\n",
    "    \n",
    "    # plot connected components\n",
    "    if plotIt:\n",
    "        fit,ax = plt.subplots(1)\n",
    "        ax.imshow(labels, cmap='nipy_spectral')\n",
    "        for i in range(len(rec)):\n",
    "            rect = mpl.patches.Rectangle(rec[i][0], rec[i][1], rec[i][2], linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "        plt.show()\n",
    "    \n",
    "    return np.array(wbLine).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clExp = pd.read_csv(\"data/letter-slope-classifications_10nov2017.csv\")\n",
    "clExp = clExp.loc[clExp[\"workflow_version\"] == 16.28]\n",
    "# len(clExp.loc[:,\"subject_ids\"].drop_duplicates())\n",
    "# len(clExp.loc)\n",
    "clExp[\"subj_json\"] = [json.loads(q) for q in clExp[\"subject_data\"]]\n",
    "clExp[\"hdl_id\"] = [q.get(list(q.keys())[0]).get(\"hdl_id\", \"\") for q in clExp[\"subj_json\"]]\n",
    "clExp = clExp.loc[clExp[\"hdl_id\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify file and folder locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensusFolder = \"C:/Users/danny/Repos/text_segmentation/consensus/consensus/\"\n",
    "# consensusFile = consensusFolder + \"decoding-the-civil-war-consensus-linewise_{mss_label}.csv\"\n",
    "subjFile = \"data/decoding-the-civil-war-subjects-9-29-17.csv\"\n",
    "savefile = \"accuracy.pkl\"\n",
    "\n",
    "consensusCsvFiles = glob.glob(\n",
    "    '{}/*.csv'.format(consensusFolder))\n",
    "consensusCsvFiles = [i for i in consensusCsvFiles if \"linewise\" in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mssEC_02_007\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n",
      "mssEC_02_032\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n",
      "mssEC_02_131\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n",
      "mssEC_02_175\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n",
      "mssEC_04_089\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n",
      "mssEC_11_085\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n",
      "mssEC_11_092\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n",
      "mssEC_11_120\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
      "mssEC_11_194\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n",
      "mssEC_11_386\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \n",
      "mssEC_22_170\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n",
      "mssEC_35_097\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n"
     ]
    }
   ],
   "source": [
    "for consensusFile in consensusCsvFiles:\n",
    "    ##############################################################################################\n",
    "    # Read data files\n",
    "    # subject file\n",
    "    subj = pd.read_csv(subjFile)\n",
    "    # get only the workflow from the live project\n",
    "    subj = subj.loc[subj['workflow_id'] == 1874]\n",
    "\n",
    "    # get metadata in dictionary format\n",
    "    subj[\"meta_json\"] = [json.loads(q) for q in subj[\"metadata\"]]\n",
    "    # get hdl_id from metadata\n",
    "    subj[\"hdl_id\"] = [q.get(\"hdl_id\", \"mssF\") for q in subj[\"meta_json\"]]\n",
    "    # get image url\n",
    "    subj[\"url\"] = [json.loads(q).get(\"0\") for q in subj[\"locations\"]]\n",
    "\n",
    "    # remove images without ids\n",
    "    subj = subj[subj[\"hdl_id\"] != \"mssF\"]\n",
    "    # remove codebook images (mssEC_36-67)\n",
    "    filt = subj[\"hdl_id\"].str.contains(\"mssEC_3[6-9]|[4-6][0-9]\")\n",
    "    subj = subj[~filt]\n",
    "    # remove ledgers that seemed weird (only easy stuff for now)\n",
    "    filt = subj[\"hdl_id\"].str.contains(\"mssEC_3[0-3]|2[6-9]\")\n",
    "    subj = subj[~filt]\n",
    "    # remove the first few pages because they tended to be blank\n",
    "    filt = subj[\"hdl_id\"].str.contains(\"mssEC_\\d\\d_00[1-6]\")\n",
    "    subj = subj[~filt]\n",
    "\n",
    "    # consensus file (by line)\n",
    "    cons = pd.read_csv(consensusFile, sep=\"@@\", engine=\"python\").drop_duplicates()\n",
    "\n",
    "    # combine the two and sift out unneeded columns\n",
    "    allTelegramInfo = pd.merge(cons, subj, on=\"hdl_id\", suffixes=[\"_cons\", \"_subj\"])\n",
    "    idAndUrl = allTelegramInfo.loc[:,[\"hdl_id\", \"url_cons\"]].drop_duplicates()\n",
    "    transcriptionsByLine = allTelegramInfo.loc[:,[\"hdl_id\", \"bestLineIndex\", \"consensus_text\", \n",
    "                                           \"y_loc\", \"len_wordlist\"]]\n",
    "\n",
    "    # use only the data that has slant information\n",
    "    idAndUrl = idAndUrl.loc[idAndUrl['hdl_id'].isin(clExp['hdl_id'])]\n",
    "    transcriptionsByLine = transcriptionsByLine.loc[transcriptionsByLine['hdl_id'].isin(clExp['hdl_id'])]\n",
    "    \n",
    "    ##############################################################################################\n",
    "    # collect the data for the wordbreaks\n",
    "    data = {}\n",
    "\n",
    "    nuOpt = np.arange(0.5, 6.5, 0.5) # 0.5:6.5\n",
    "    biOpt = list(range(1, 10)) # 1:10\n",
    "\n",
    "    for im in list(idAndUrl.index):\n",
    "        hdl_id = idAndUrl.loc[im, \"hdl_id\"]\n",
    "        data[hdl_id] = {}\n",
    "        print(hdl_id)\n",
    "\n",
    "        data[hdl_id][\"url\"] = idAndUrl.loc[im, \"url_cons\"]\n",
    "        let, grey = readImg(idAndUrl.loc[im, \"url_cons\"])#, True)\n",
    "        linesForTele = transcriptionsByLine.loc[transcriptionsByLine[\"hdl_id\"] == hdl_id]\n",
    "        linesForTele.loc[:,\"y1\"] = [eval(l)[0] for l in linesForTele.loc[:,\"y_loc\"]]\n",
    "\n",
    "\n",
    "        # read in and do all pre-processing #################################\n",
    "        let, grey = removeEdges(let, grey, rmThresh=0)\n",
    "        grey = whitenEdgesProject(grey)\n",
    "        greyBi = binarizeImg(grey, biThresh=\"otsu\")#, plotIt=True)\n",
    "        greySm = smoothImg(grey, smoothSigma=10.0)#, plotIt=True)\n",
    "        greyBiSm = smoothImg(greyBi, smoothSigma=5.0)#, plotIt=True)\n",
    "\n",
    "\n",
    "        # get linebreaks ####################################################\n",
    "        matchlim = 30\n",
    "        lbold = projBreaks(greySm, \"y\")\n",
    "        lb = []\n",
    "        lb.append(lbold[0])\n",
    "        cur = 0\n",
    "        for i in range(1, len(lbold)):\n",
    "            if np.abs(lb[cur] - lbold[i]) < matchlim:\n",
    "                lb[cur] = np.mean([lb[cur], lbold[i]])\n",
    "            else:\n",
    "                cur += 1\n",
    "                lb.append(lbold[i])\n",
    "        lb = np.array(lb).astype(\"int\")\n",
    "        data[hdl_id][\"lb\"] = lb\n",
    "\n",
    "        # get matching lines with actual lines ##############################\n",
    "        actuallb = linesForTele.loc[:, \"y1\"]\n",
    "        matches = []\n",
    "        for i in range(len(lb)):\n",
    "            closest = np.argmin(np.abs(np.subtract(lb[i], actuallb)))\n",
    "            if np.abs(lb[i] - actuallb[closest]) < matchlim:\n",
    "                matches.append(closest)\n",
    "            else:\n",
    "                matches.append(-1)\n",
    "        data[hdl_id][\"matches\"] = matches\n",
    "\n",
    "        # get wordbreaks ####################################################\n",
    "        accurL = []\n",
    "\n",
    "        for i in range(1, len(lb)):\n",
    "            print(i, end=\" \")\n",
    "            if matches[i] == -1:\n",
    "                continue\n",
    "            chunk = grey[lb[i-1]:lb[i],]\n",
    "            rw = linesForTele.loc[matches[i],\"consensus_text\"]\n",
    "            # remove leading and trailing \"\n",
    "            if rw[0] == '\"':\n",
    "                rw = rw[1:]\n",
    "            if rw[-1] == '\"':\n",
    "                rw = rw[:-1]\n",
    "            nr = len(rw.split(\" \"))\n",
    "            for nu in nuOpt:\n",
    "                for bi in biOpt:\n",
    "                    br = gaussBreaks(chunk, nu=nu, biThresh=bi, shear=0, fix=0)\n",
    "                    accurL.append({\"i\":i, \"nu\":nu, \"bi\":bi, \"nr\":nr, \"br\":br})\n",
    "        accur = pd.DataFrame(accurL)\n",
    "        data[hdl_id][\"segment\"] = accur\n",
    "        print()\n",
    "        \n",
    "        # save object\n",
    "        with open(savefile, \"wb\") as f:\n",
    "            pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOOOOOOOOOOOOOOOOOOOOOOOOOOLLLLLLLLLLLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clExp = pd.read_csv(\"data/letter-slope-classifications_10nov2017.csv\")\n",
    "clExp = clExp.loc[clExp[\"workflow_version\"] == 16.28]\n",
    "# len(clExp.loc[:,\"subject_ids\"].drop_duplicates())\n",
    "# len(clExp.loc)\n",
    "clExp[\"subj_json\"] = [json.loads(q) for q in clExp[\"subject_data\"]]\n",
    "clExp[\"hdl_id\"] = [q.get(list(q.keys())[0]).get(\"hdl_id\", \"\") for q in clExp[\"subj_json\"]]\n",
    "clExp = clExp.loc[clExp[\"hdl_id\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify file and folder locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consensusFolder = \"C:/Users/danny/Repos/text_segmentation/consensus/consensus/\"\n",
    "consensusFile = consensusFolder + \"decoding-the-civil-war-consensus-linewise_{mss_label}.csv\"\n",
    "subjFile = \"dcw_data/decoding-the-civil-war-subjects-9-29-17.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will late be in a loop - get hdl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mssLabel = \"fication_export_mssEC_02_05_25_17\"\n",
    "consensusFile = consensusFile.format(mss_label=mssLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subject file\n",
    "subj = pd.read_csv(subjFile)\n",
    "# get only the workflow from the live project\n",
    "subj = subj.loc[subj['workflow_id'] == 1874]\n",
    "\n",
    "# get metadata in dictionary format\n",
    "subj[\"meta_json\"] = [json.loads(q) for q in subj[\"metadata\"]]\n",
    "# get hdl_id from metadata\n",
    "subj[\"hdl_id\"] = [q.get(\"hdl_id\", \"mssF\") for q in subj[\"meta_json\"]]\n",
    "# get image url\n",
    "subj[\"url\"] = [json.loads(q).get(\"0\") for q in subj[\"locations\"]]\n",
    "\n",
    "# remove images without ids\n",
    "subj = subj[subj[\"hdl_id\"] != \"mssF\"]\n",
    "# remove codebook images (mssEC_36-67)\n",
    "filt = subj[\"hdl_id\"].str.contains(\"mssEC_3[6-9]|[4-6][0-9]\")\n",
    "subj = subj[~filt]\n",
    "# remove ledgers that seemed weird (only easy stuff for now)\n",
    "filt = subj[\"hdl_id\"].str.contains(\"mssEC_3[0-3]|2[6-9]\")\n",
    "subj = subj[~filt]\n",
    "# remove the first few pages because they tended to be blank\n",
    "filt = subj[\"hdl_id\"].str.contains(\"mssEC_\\d\\d_00[1-6]\")\n",
    "subj = subj[~filt]\n",
    "\n",
    "# consensus file (by line)\n",
    "cons = pd.read_csv(consensusFile, sep=\"@@\", engine=\"python\").drop_duplicates()\n",
    "\n",
    "# combine the two and sift out unneeded columns\n",
    "allTelegramInfo = pd.merge(cons, subj, on=\"hdl_id\", suffixes=[\"_cons\", \"_subj\"])\n",
    "idAndUrl = allTelegramInfo.loc[:,[\"hdl_id\", \"url_cons\"]].drop_duplicates()\n",
    "transcriptionsByLine = allTelegramInfo.loc[:,[\"hdl_id\", \"bestLineIndex\", \"consensus_text\", \n",
    "                                       \"y_loc\", \"len_wordlist\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use only the data that has slant information\n",
    "idAndUrl = idAndUrl.loc[idAndUrl['hdl_id'].isin(clExp['hdl_id'])]\n",
    "transcriptionsByLine = transcriptionsByLine.loc[transcriptionsByLine['hdl_id'].isin(clExp['hdl_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect the data for the wordbreaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mssEC_02_007\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 \n",
      "mssEC_02_032\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n",
      "mssEC_02_131\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 \n",
      "mssEC_02_175\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "nuOpt = np.arange(3, 5, 0.5) # 0.5:6.5\n",
    "biOpt = list(range(1, 4)) # 1:10\n",
    "\n",
    "for im in list(idAndUrl.index):\n",
    "    hdl_id = idAndUrl.loc[im, \"hdl_id\"]\n",
    "    data[hdl_id] = {}\n",
    "    print(hdl_id)\n",
    "    \n",
    "    let, grey = readImg(idAndUrl.loc[im, \"url_cons\"])#, True)\n",
    "    linesForTele = transcriptionsByLine.loc[transcriptionsByLine[\"hdl_id\"] == hdl_id]\n",
    "    linesForTele.loc[:,\"y1\"] = [eval(l)[0] for l in linesForTele.loc[:,\"y_loc\"]]\n",
    "\n",
    "\n",
    "    # read in and do all pre-processing #################################\n",
    "    let, grey = removeEdges(let, grey, rmThresh=0)\n",
    "    grey = whitenEdgesProject(grey)\n",
    "    greyBi = binarizeImg(grey, biThresh=\"otsu\")#, plotIt=True)\n",
    "    greySm = smoothImg(grey, smoothSigma=10.0)#, plotIt=True)\n",
    "    greyBiSm = smoothImg(greyBi, smoothSigma=5.0)#, plotIt=True)\n",
    "\n",
    "\n",
    "    # get linebreaks ####################################################\n",
    "    matchlim = 30\n",
    "    lbold = projBreaks(greySm, \"y\")\n",
    "    lb = []\n",
    "    lb.append(lbold[0])\n",
    "    cur = 0\n",
    "    for i in range(1, len(lbold)):\n",
    "        if np.abs(lb[cur] - lbold[i]) < matchlim:\n",
    "            lb[cur] = np.mean([lb[cur], lbold[i]])\n",
    "        else:\n",
    "            cur += 1\n",
    "            lb.append(lbold[i])\n",
    "    lb = np.array(lb).astype(\"int\")\n",
    "    data[hdl_id][\"lb\"] = lb\n",
    "    \n",
    "    # get matching lines with actual lines ##############################\n",
    "    actuallb = linesForTele.loc[:, \"y1\"]\n",
    "    matches = []\n",
    "    for i in range(len(lb)):\n",
    "        closest = np.argmin(np.abs(np.subtract(lb[i], actuallb)))\n",
    "        if np.abs(lb[i] - actuallb[closest]) < matchlim:\n",
    "            matches.append(closest)\n",
    "        else:\n",
    "            matches.append(-1)\n",
    "    data[hdl_id][\"matches\"] = matches\n",
    "    \n",
    "    # get wordbreaks ####################################################\n",
    "    accurL = []\n",
    "\n",
    "    for i in range(1, len(lb)):\n",
    "        print(i, end=\" \")\n",
    "        if matches[i] == -1:\n",
    "            continue\n",
    "        chunk = grey[lb[i-1]:lb[i],]\n",
    "        nr = len(eval(linesForTele.loc[matches[i],\"consensus_text\"]).split(\" \"))\n",
    "        for nu in nuOpt:\n",
    "            for bi in biOpt:\n",
    "                br = gaussBreaks(chunk, nu=nu, biThresh=bi, shear=0, fix=0)\n",
    "                accurL.append({\"i\":i, \"nu\":nu, \"bi\":bi, \"nr\":nr, \"br\":br})\n",
    "    accur = pd.DataFrame(accurL)\n",
    "    data[hdl_id][\"segment\"] = accur\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
