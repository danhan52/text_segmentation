{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load slops from zooniverse\n",
    "Based on [this code](https://github.com/zooniverse/Data-digging/blob/master/example_scripts/galaxy_zoo_bar_lengths/extract_line_drawings.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation code we've been using has some weird behaviors and might have\n",
    "a bug. I didn't write that code and I can't easily track down the bug. It\n",
    "may be required that we aggregate the bar lengths ourselves. I'd prefer not to,\n",
    "but nevertheless I'm moving forward here.\n",
    "\n",
    "The purpose of this code is to read in the raw classifications and extract\n",
    "just the line drawing marks, then export them to a csv file with one row per\n",
    "subject-line-drawing pair.\n",
    "\n",
    "We collected line drawings in two workflows, with ids:\n",
    "workflow_id == 3, workflow_version == 56.13\n",
    "workflow_id == 1422, workflow_version == 10.8\n",
    "\n",
    "so we'll only worry about those, but we'll treat them as the same here once we\n",
    "extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# define some stuff\n",
    "#####################################\n",
    "\n",
    "classfile_in = \"data/letter-slope-classifications.csv\"\n",
    "markfile_out = \"data/slope-lines.csv\"\n",
    "\n",
    "workflow_ids = [3, 1422]\n",
    "workflow_int_versions = [56, 10]\n",
    "# also the drawing task is the fourth task in workflow 3 and the initial task in workflow 1422\n",
    "workflow_int_tasks = [3, 0]\n",
    "\n",
    "# a function we will use later, extracting a particular task from a classification\n",
    "# and failing gracefully if that task doesn\"t exist\n",
    "def get_marks(q, i):\n",
    "    try:\n",
    "        return q[i]\n",
    "    except:\n",
    "        return \"[]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# read the file\n",
    "#####################################\n",
    "# the memory parameter is because this is a big file and some of the columns take a bit more processing, so I\"m telling pandas not to take a shortcut\n",
    "cla = pd.read_csv(classfile_in, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we only care about the major workflow version (e.g. \"56\" from 56.13)\n",
    "# casting to integer doesn\"t round, it just cuts off the decimal, i.e. what we want\n",
    "cla[\"workflow_major\"] = cla.workflow_version.astype(int)\n",
    "\n",
    "# make annotations (the actual classification content) something more readable - the current column is just a string but it\"s formatted to be read as a json/list\n",
    "cla[\"anno_json\"] = [json.loads(q) for q in cla.annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'task': u'T0',\n",
       "  u'task_label': u'Is there more than one line of text in the image?',\n",
       "  u'value': u'Yes'},\n",
       " {u'task': u'T1',\n",
       "  u'task_label': u'Draw a single line that follows the slant of the handwriting. See help for examples.',\n",
       "  u'value': [{u'details': [],\n",
       "    u'frame': 0,\n",
       "    u'tool': 0,\n",
       "    u'tool_label': u'Slope line',\n",
       "    u'x1': 392.0789794921875,\n",
       "    u'x2': 199.34486389160156,\n",
       "    u'y1': 6.795112609863281,\n",
       "    u'y2': 193.35186767578125},\n",
       "   {u'details': [],\n",
       "    u'frame': 0,\n",
       "    u'tool': 0,\n",
       "    u'tool_label': u'Slope line',\n",
       "    u'x1': 1286.56298828125,\n",
       "    u'x2': 1087.6514892578125,\n",
       "    u'y1': 12.972489356994629,\n",
       "    u'y2': 203.23565673828125}]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cla[\"anno_json\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are going to extract the line drawings from each workflow and then combine the 2 workflows together again\n",
    "\n",
    "for i_w, wid in enumerate(workflow_ids):\n",
    "    # get the line markings based on which task is right for this workflow_id\n",
    "    cla[\"lines\"] = [get_marks(q, workflow_int_tasks[i_w]) for q in cla.anno_json]\n",
    "\n",
    "    # identify the rows where the classification is in the correct workflow & version\n",
    "    relevant_classifications = (cla.workflow_id == wid) & (cla.workflow_major == workflow_int_versions[i_w])\n",
    "\n",
    "    # I\"m sure there\"s a better way to do this but this ... should... work for now\n",
    "    # assuming .copy() actually works.\n",
    "    if i_w == 0:\n",
    "        first_class = cla[relevant_classifications].copy()\n",
    "    else:\n",
    "        second_class = cla[relevant_classifications].copy()\n",
    "\n",
    "\n",
    "# now join the 2 subsets\n",
    "both = [first_class, second_class]\n",
    "\n",
    "both_class_all = pd.concat(both)\n",
    "\n",
    "# now let\"s not worry about the classifications where no lines were drawn\n",
    "# to identify which aren\"t empty we need to parse each row (loop) and also force the lines column value to be read as a string\n",
    "has_marks = [len(q) > 3 for q in both_class_all.lines.astype(str)]\n",
    "\n",
    "both_class = both_class_all[has_marks]\n",
    "\n",
    "#In [117]: len(both_class)\n",
    "#Out[117]: 65088\n",
    "\n",
    "# Now we need to go through each row of this dataframe and break out individual line marks into separate rows. I don\"t see any way around this requiring a for loop.\n",
    "\n",
    "# initialize the dict so we can add to it later\n",
    "line_marks = 0\n",
    "line_marks = pd.DataFrame()\n",
    "\n",
    "# also we\"ll use this to track unique marking ids\n",
    "mark_id = 0\n",
    "\n",
    "# loop through each row in both_class\n",
    "# this seems so very unpythonic\n",
    "for i, row in enumerate(both_class.iterrows()):\n",
    "    thelines = row[1].lines[\"value\"]\n",
    "    # loop through each individual line marking\n",
    "    for themark in thelines:\n",
    "        line_mark_class = {}\n",
    "        line_mark_class[\"mark_id\"] = mark_id\n",
    "        line_mark_class[\"classification_id\"] = row[1][\"classification_id\"]\n",
    "        line_mark_class[\"subject_id\"] = row[1][\"subject_id\"]\n",
    "        line_mark_class[\"user_name\"] = row[1][\"user_name\"]\n",
    "        line_mark_class[\"user_id\"] = row[1][\"user_id\"]\n",
    "        line_mark_class[\"user_ip\"] = row[1][\"user_ip\"]\n",
    "        line_mark_class[\"created_at\"] = row[1][\"created_at\"]\n",
    "        line_mark_class[\"workflow_id\"] = row[1][\"workflow_id\"]\n",
    "        line_mark_class[\"workflow_version\"] = row[1][\"workflow_version\"]\n",
    "        line_mark_class[\"x1\"] = themark[\"x1\"]\n",
    "        line_mark_class[\"y1\"] = themark[\"y1\"]\n",
    "        line_mark_class[\"x2\"] = themark[\"x2\"]\n",
    "        line_mark_class[\"y2\"] = themark[\"y2\"]\n",
    "        line_mark_class[\"i_tool\"] = themark[\"tool\"]\n",
    "\n",
    "        qq = pd.Series(line_mark_class)\n",
    "\n",
    "        if len(line_marks) == 0:\n",
    "            line_marks = pd.DataFrame(qq).T\n",
    "        else:\n",
    "            line_marks = pd.concat([line_marks, pd.DataFrame(qq).T])\n",
    "\n",
    "        mark_id +=1\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print(\"Mark %d recorded at %s ...\" % (i, str(datetime.now())))\n",
    "\n",
    "# Note: the above is VERY slow and each iteration takes longer the farther in the loop you are. I\"d probably have better results manually writing each line to a csv file and then re-reading it in below.\n",
    "\n",
    "\n",
    "line_marks.set_index(\"mark_id\", inplace=True)\n",
    "\n",
    "# compute the slope and intercepts of each line\n",
    "# m = (y2-y1)/(x2-x1) with some stuff to make sure we\"re not dividing by integers\n",
    "line_marks[\"slope\"] = (line_marks.y2.astype(float) - line_marks.y1.astype(float)) / (line_marks.x2.astype(float) - line_marks.x1.astype(float))\n",
    "# b = y1 - m*x1\n",
    "line_marks[\"intercept\"] = line_marks.y1.astype(float) - (line_marks.slope * line_marks.x1.astype(float))\n",
    "\n",
    "# compute the length of each line - this is just the pythagorean theorem\n",
    "dist_x = line_marks.x1.astype(float) - line_marks.x2.astype(float)\n",
    "dist_y = line_marks.y1.astype(float) - line_marks.y2.astype(float)\n",
    "len2 = (dist_x*dist_x) + (dist_y*dist_y)\n",
    "line_marks[\"length\"] = [np.sqrt(q) for q in len2]\n",
    "\n",
    "# this sets not just which columns are printed but also the order they\"re printed in\n",
    "# note the mark_id column was set as the index so will be printed as column 1 and doesn\"t need an extra mention here\n",
    "columns_toprint = \"classification_id subject_id user_name user_id user_ip created_at workflow_id workflow_version x1 x2 y1 y2 slope intercept length i_tool\".split()\n",
    "line_marks[columns_toprint].to_csv(markfile_out)\n",
    "\n",
    "\n",
    "# # I paste stuff like this into ipython to help me figure out the structure of the beast\n",
    "# # this is how I knew that all the important stuff in each row is in row[1],\n",
    "# # how I knew that the important stuff in the \"lines\" column was then under [\"value\"]\n",
    "# # etc.\n",
    "# #\n",
    "# # Note you have to type %paste into ipython instead of actually pasting, because otherwise the line indents don\"t get read properly\n",
    "# for i, row in enumerate(both_class.iterrows()):\n",
    "#     print(i)\n",
    "#     print(row)\n",
    "#     print(row[1].lines)\n",
    "#     print(\"Marcooooooo\")\n",
    "#     markings = row[1].lines[\"value\"]\n",
    "#     for themark in markings:\n",
    "#         print(themark)\n",
    "#         print(\"holla\")\n",
    "#     print(\"Poloooooooo\")\n",
    "#     if (i > 3):\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
